# Meme Clustering Project

This project focuses on clustering memes using various image features and clustering algorithms.

## Project Structure

```
.
├── README.md
├── adjacency_matrices
│   ├── combined
│   │   ├── adjacency_matrix.npz
│   │   └── script.py
│   ├── global_only
│   │   ├── adjacency_matrix.npz
│   │   └── script.py
│   ├── individual_features
│   │   ├── create_matrix.py
│   │   ├── combine.py
│   │   ├── HISTO-VIT-BERT-PHASH-CLIP_TEXT-CLIP_IMAGE-SURF-FACES-SURF_NO_TEXT-BERT_NGRAMS.pkl
│   │   └── matrix_info
│   │       └── VIT.json
│   ├── local_only
│   │   ├── adjacency_matrix.npz
│   │   └── script.py
│   └── vit_only
│       ├── adjacency_matrix.npz
│       └── script.py
├── clustering
│   ├── standard
│   │   ├── cluster.py
│   │   ├── combined_5000.json
│   │   ├── combined_8500.json
│   │   ├── combined_11000.json
│   │   ├── global_only_5000.json
│   │   ├── global_only_8500.json
│   │   ├── global_only_11000.json
│   │   ├── local_only_5000.json
│   │   ├── local_only_8500.json
│   │   ├── local_only_11000.json
│   │   ├── vit_only_5000.json
│   │   ├── vit_only_8500.json
│   │   └── vit_only_11000.json
│   └── templatic
│       ├── cluster.py
│       ├── combined_5000.json
│       ├── combined_8500.json
│       ├── combined_11000.json
│       ├── global_only_5000.json
│       ├── global_only_8500.json
│       ├── global_only_11000.json
│       ├── local_only_5000.json
│       ├── local_only_8500.json
│       ├── local_only_11000.json
│       ├── vit_only_5000.json
│       ├── vit_only_8500.json
│       └── vit_only_11000.json
└── consistency.py
```

## File Descriptions

### `consistency.py`

This script calculates and prints a table of consistency metrics for different clustering configurations. It reads cluster data (JSON files) from the `clustering/standard/` and `clustering/templatic/` directories and evaluates how consistent the clusters are, particularly focusing on images from "Know Your Meme" (KYM).

### `adjacency_matrices/`

This directory stores adjacency matrices used for clustering. Each subdirectory represents a different combination of features.

*   **`adjacency_matrices/individual_features/`**: This directory contains scripts and data for generating and combining adjacency matrices for individual features.
    *   `create_matrix.py`: This script is used to generate an individual adjacency matrix (as an `.npz` file, e.g., `VIT.npz`) for a specific feature set. It requires a Faiss index file (e.g., `faiss/indices/{FEATURE_SET}.idx`) for the corresponding features and associated raw feature data (e.g., `.pkl` files containing feature vectors). You need to configure and run this script for each feature set you want to include.
    *   `combine.py`: This script takes all the individual feature adjacency matrices (`.npz` files generated by `create_matrix.py`) and combines them into a single Python pickle file (`HISTO-VIT-BERT-PHASH-CLIP_TEXT-CLIP_IMAGE-SURF-FACES-SURF_NO_TEXT-BERT_NGRAMS.pkl`). This `.pkl` file stores a dictionary where keys are feature set names and values are their corresponding sparse adjacency matrices. The script uses JSON files in `matrix_info/` (e.g., `VIT.json`, `BERT.json`) to map image IDs to URLs, allowing it to align images correctly across different feature sets before saving the combined dictionary.
    *   `HISTO-VIT-BERT-PHASH-CLIP_TEXT-CLIP_IMAGE-SURF-FACES-SURF_NO_TEXT-BERT_NGRAMS.pkl`: The pickled dictionary file created by `combine.py`. It contains pre-calculated and aligned adjacency matrices for individual image features. This dictionary is the primary input for the scripts in the subdirectories like `combined/`, `global_only/`, etc.
    *   `matrix_info/`: This directory should contain JSON files (e.g., `VIT.json`, `BERT.json`) for each feature set. These files map internal index IDs to image URLs and other metadata. The `adjacency_matrices/individual_features/combine.py` script uses these files to align images across different feature sets before creating the combined `.pkl` dictionary.

*   **`adjacency_matrices/<feature_set>/`** (e.g., `combined/`, `global_only/`, `local_only/`, `vit_only/`):
    *   `adjacency_matrix.npz`: A NumPy sparse matrix representing the final (e.g., combined and/or normalized) adjacency matrix for the specific feature set or combination.
    *   `script.py`: A Python script that loads the main dictionary from `adjacency_matrices/individual_features/HISTO-VIT-BERT-PHASH-CLIP_TEXT-CLIP_IMAGE-SURF-FACES-SURF_NO_TEXT-BERT_NGRAMS.pkl`. It then selects specific feature matrices from this dictionary, normalizes them (e.g., Z-score normalization, shifting mean), and then either combines (sums) them (e.g., in `combined/script.py`, `global_only/script.py`, `local_only/script.py`) or uses a single one directly (e.g., `vit_only/script.py` extracts the 'VIT' matrix for convenience) to produce the `adjacency_matrix.npz` for that specific configuration.

### `clustering/`

This directory contains the results of different clustering approaches.

*   **`clustering/standard/`**:
    *   `cluster.py`: This script performs community detection (specifically Louvain algorithm) on the adjacency matrices found in `adjacency_matrices/<feature_set>/adjacency_matrix.npz`. It iterates through different feature sets (`combined`, `local_only`, `global_only`, `vit_only`) and different target numbers of clustered images (5000, 8500, 11000). It uses a binary search to find an appropriate similarity threshold to achieve the target number of images. The script then extracts cluster information (members, platform counts, template counts, URLs) and saves the results as JSON files.
    *   `*.json` files (e.g., `combined_5000.json`): These JSON files store the output of the `clustering/standard/cluster.py` script. Each file contains a list of clusters, where each cluster has an ID, a list of member image indices, platform statistics (Reddit, KYM), and template statistics.

*   **`clustering/templatic/`**:
    *   `cluster.py`: This script implements a templatic clustering approach. It starts with an initial set of template clusters (loaded from `clustering/standard/{set_title}_5000.json`). It then iterates through the remaining images and assigns them to the most similar existing template cluster if the similarity (based on the adjacency matrix from `adjacency_matrices/<feature_set>/adjacency_matrix.npz`) exceeds a dynamically determined threshold. This threshold is found using a binary search to match a target number of total clustered images (5000, 8500, 11000). The script updates the cluster information and saves the results as JSON files.
    *   `*.json` files (e.g., `combined_5000.json`): These JSON files store the output of the `clustering/templatic/cluster.py` script, with a similar structure to the files in `clustering/standard/` but reflecting the templatic clustering results.

## How to Run

The process involves several steps to generate adjacency matrices and then perform clustering:

1.  **Generate Individual Feature Adjacency Matrices**:
    *   For each feature set (e.g., 'VIT', 'HISTO', 'BERT_NGRAMS'), you first need to create an individual adjacency matrix.
    *   Modify the `FEATURE_SET` variable and file paths (for Faiss index and raw feature data) in `adjacency_matrices/individual_features/create_matrix.py` to the desired feature set.
    *   Ensure you have the corresponding Faiss index file (e.g., `faiss/indices/VIT.idx`) and any necessary raw feature data files (e.g., `kym_bert_ngrams.pkl`, `reddit_bert_ngrams.pkl` as referenced in `create_matrix.py`).
    *   Run the script: `python adjacency_matrices/individual_features/create_matrix.py`.
    *   This will output an `.npz` file for that feature set (e.g., `adjacency_matrices/individual_features/VIT.npz`).
    *   Repeat this for all desired individual feature sets. Ensure a corresponding `matrix_info/{FEATURE_SET}.json` file exists for each feature set that will be processed by `combine.py`.

2.  **Combine Individual Matrices into a Single Dictionary File**:
    *   Once all individual feature adjacency matrices (`.npz` files) are generated in `adjacency_matrices/individual_features/`, and their corresponding `matrix_info/{FEATURE_SET}.json` files are present, run the `combine.py` script:
        `python adjacency_matrices/individual_features/combine.py`
    *   This script reads the individual `.npz` files, aligns them based on image URLs (using information from `matrix_info/` files), and saves them as a dictionary in `adjacency_matrices/individual_features/HISTO-VIT-BERT-PHASH-CLIP_TEXT-CLIP_IMAGE-SURF-FACES-SURF_NO_TEXT-BERT_NGRAMS.pkl`.

3.  **Generate Specific Combined/Normalized Adjacency Matrices**:
    *   The scripts in subdirectories like `adjacency_matrices/combined/`, `adjacency_matrices/global_only/`, `adjacency_matrices/local_only/`, and `adjacency_matrices/vit_only/` use the `.pkl` dictionary generated in the previous step.
    *   These scripts load the dictionary, select specific feature matrices, normalize them, and then either combine (sum) them or use them directly to create a final `adjacency_matrix.npz` for that specific configuration.
        *   For example, `adjacency_matrices/combined/script.py` sums multiple feature matrices.
        *   `adjacency_matrices/global_only/script.py` sums a subset of global features.
        *   `adjacency_matrices/local_only/script.py` sums a subset of local features.
        *   `adjacency_matrices/vit_only/script.py` extracts only the 'VIT' matrix from the dictionary (out of convenience, as it's already processed and aligned).
    *   Run the respective script, e.g.:
        `python adjacency_matrices/combined/script.py`
        `python adjacency_matrices/global_only/script.py`
        `python adjacency_matrices/local_only/script.py`
        `python adjacency_matrices/vit_only/script.py`

4.  **Perform Clustering**:
    *   To run standard Louvain clustering: `python clustering/standard/cluster.py`
    *   To run templatic clustering: `python clustering/templatic/cluster.py`
    *   These scripts will use the `adjacency_matrix.npz` files generated in step 3 and will create JSON cluster files in their respective directories.

5.  **Calculate Consistency Metrics**:
    *   Run `python consistency.py` to calculate and display the consistency metrics based on the generated cluster JSON files.

## Dependencies

The project relies on several Python libraries:

*   numpy
*   scipy
*   networkx
*   python-louvain (for community detection)
*   matplotlib (for plotting, used in `consistency.py`)
*   faiss (for `create_matrix.py`)

Ensure these are installed in your Python environment. 
